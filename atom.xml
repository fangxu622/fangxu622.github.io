<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Keep It Simple Do It Well</title>
  
  <subtitle>Fang Xu</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-07-24T12:18:52.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Fang Xu</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2020/01/26/hello-world/"/>
    <id>http://yoursite.com/2020/01/26/hello-world/</id>
    <published>2020-01-26T09:18:08.360Z</published>
    <updated>2018-07-24T12:18:52.000Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><a id="more"></a><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre><code class="bash">$ hexo new &quot;My New Post&quot;</code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre><code class="bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre><code class="bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre><code class="bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;documentation&lt;/a&gt; for more info. If you get any problems when using Hexo, you can find the answer in &lt;a href=&quot;https://hexo.io/docs/troubleshooting.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;troubleshooting&lt;/a&gt; or you can ask me on &lt;a href=&quot;https://github.com/hexojs/hexo/issues&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>TensorFlow模型保存与恢复</title>
    <link href="http://yoursite.com/2018/09/30/tensorflow%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E4%B8%8E%E6%81%A2%E5%A4%8D/"/>
    <id>http://yoursite.com/2018/09/30/tensorflow模型保存与恢复/</id>
    <published>2018-09-30T05:48:05.000Z</published>
    <updated>2018-10-08T10:46:35.414Z</updated>
    
    <content type="html"><![CDATA[<p>在本教程中，我将会解释：</p><p>1.TensorFlow模型是什么样的？</p><p>2.如何保存TensorFlow模型？</p><p>3.如何恢复预测/转移学习的TensorFlow模型？</p><p>4.如何使用导入的预先训练的模型进行微调和修改?</p><p>5.这个教程假设你已经对神经网络有了一定的了解。如果不了解的话请查阅相关资料</p><a id="more"></a><p>1.保存</p><p>将训练好的模型参数保存起来，以便以后进行验证或测试。tf里面提供模型保存的是tf.train.Saver()模块。<br>    saver=tf.train.Saver()</p><p>在创建这个Saver对象的时候，有一个参数经常会用到，max_to_keep 参数，这个是用来设置保存模型的个数，默认为5，即 max_to_keep=5，保存最近的5个模型。如果想每训练一代（epoch)就想保存一次模型，则可以将 max_to_keep设置为None或者0，但是这样做除了多占用硬盘，并没有实际多大的用处，因此不推荐，如</p><pre><code>import tensorflow as tfimport numpy as npx = tf.placeholder(tf.float32, shape=[None, 1])y = 4 * x + 4w = tf.Variable(tf.random_normal([1], -1, 1))b = tf.Variable(tf.zeros([1]))y_predict = w * x + bloss = tf.reduce_mean(tf.square(y - y_predict))optimizer = tf.train.GradientDescentOptimizer(0.5)train = optimizer.minimize(loss)isTrain = Falsetrain_steps = 100checkpoint_steps = 50checkpoint_dir = &#39;&#39;saver = tf.train.Saver()  # defaults to saving all variables - in this case w and bx_data = np.reshape(np.random.rand(10).astype(np.float32), (10, 1))with tf.Session() as sess:    sess.run(tf.initialize_all_variables())    if isTrain:        for i in xrange(train_steps):            sess.run(train, feed_dict={x: x_data})            if (i + 1) % checkpoint_steps == 0:                saver.save(sess, checkpoint_dir + &#39;model.ckpt&#39;, global_step=i+1)    else:        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)        if ckpt and ckpt.model_checkpoint_path:            saver.restore(sess, ckpt.model_checkpoint_path)        else:            pass        print(sess.run(w))        print(sess.run(b))</code></pre><p>模型保存系列<br><a href="https://blog.csdn.net/tan_handsome/article/details/79303269" target="_blank" rel="noopener">https://blog.csdn.net/tan_handsome/article/details/79303269</a></p><p>【tensorflow】tf.train.get_checkpoint_state<br><a href="https://blog.csdn.net/changeforeve/article/details/80268522" target="_blank" rel="noopener">https://blog.csdn.net/changeforeve/article/details/80268522</a></p><p>tf.Summary 系列<br><a href="https://www.cnblogs.com/lyc-seu/p/8647792.html" target="_blank" rel="noopener">https://www.cnblogs.com/lyc-seu/p/8647792.html</a></p><p>训练了一个神经网络之后，我们希望保存它以便将来使用。那么什么是TensorFlow模型?Tensorflow模型主要包含我们所培训的网络参数的网络设计或图形和值。因此，Tensorflow模型有两个主要的文件:</p><p><code>import tensorflow as tfhello = tf.constant(&#39;Hello, TensorFlow!&#39;)sess = tf.Session()print sess.run(hello)</code></p><p>首先，通过tf.constant创建一个常量，然后启动Tensorflow的Session，调用sess的run方法来启动整个graph。 接下来我们做下简单的数学的方法：</p><pre><code>import tensorflow as tfa = tf.constant(2)b = tf.constant(3)with tf.Session() as sess:    print &quot;a=2, b=3&quot;    print &quot;Addition with constants: %i&quot; % sess.run(a+b)    print &quot;Multiplication with constants: %i&quot; % sess.run(a*b)</code></pre><p>tensorflow中保存数据的利器，placeholder（type,strucuct…)它的第一个参数是你要保存的数据的数据类型，大多数是tensorflow中的float32数据类型，后面的参数就是要保存数据的结构，比如要保存一个1×2的矩阵，则struct=[1 2]。它在使用的时候和前面的variable不同的是在session运行阶段，需要给placeholder提供数据，利用feed_dict的字典结构给placeholdr变量“喂数据”，具体使用如下</p><p>placeholder 使用</p><pre><code>import tensorflow as tfa = tf.placeholder(tf.int16)b = tf.placeholder(tf.int16)add = tf.add(a, b)mul = tf.multiply(a, b)with tf.Session() as sess:    # Run every operation with variable input    print &quot;Addition with variables: %i&quot; % sess.run(add, feed_dict={a: 2, b: 3})    print &quot;Multiplication with variables: %i&quot; % sess.run(mul, feed_dict={a: 2, b: 3})# output:Addition with variables: 5Multiplication with variables: 6*/matrix1 = tf.constant([[3., 3.]])matrix2 = tf.constant([[2.],[2.]])product=tf.matmul(matrix1,matrix2)with tf.Session() as sess:    result = sess.run(product)    print result    #result:    [[12.]]</code></pre><p>注：<br><strong>Tensorflow中废弃的API及替代<br>tf.mul  tf.sub   tf.neg 已经废弃<br>分别可用tf.multiply  tf.subtract  tf.negative替代.</strong></p><p>线性回归:以下代码来自<a href="https:#github.com/aymericdamien/TensorFlow-Examples/" target="_blank" rel="noopener">https:#github.com/aymericdamien/TensorFlow-Examples/</a><br>仅作学习用</p><pre><code>import tensorflow as tfimport numpyimport matplotlib.pyplot as pltrng = numpy.randomlearning_rate = 0.01training_epochs = 2000display_step = 50# Training Datatrain_X = numpy.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,7.042,10.791,5.313,7.997,5.654,9.27,3.1])train_Y = numpy.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,2.827,3.465,1.65,2.904,2.42,2.94,1.3])n_samples = train_X.shape[0]#tf Graph InputX = tf.placeholder(&quot;float&quot;)Y = tf.placeholder(&quot;float&quot;)#Create Model#Set model weightsW = tf.Variable(rng.randn(), name=&quot;weight&quot;)b = tf.Variable(rng.randn(), name=&quot;bias&quot;)#Construct a linear modelactivation = tf.add(tf.multiply(X, W), b)# Minimize the squared errorscost = tf.reduce_sum(tf.pow(activation-Y, 2))/(2*n_samples) #L2 lossoptimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost) #Gradient descent# Initializing the variablesinit = tf.initialize_all_variables()# Launch the graphwith tf.Session() as sess:    sess.run(init)    # Fit all training data    for epoch in range(training_epochs):        for (x, y) in zip(train_X, train_Y):            sess.run(optimizer, feed_dict={X: x, Y: y})        #Display logs per epoch step        if epoch % display_step == 0:            print &quot;Epoch:&quot;, &#39;%04d&#39; % (epoch+1), &quot;cost=&quot;, \                &quot;{:.9f}&quot;.format(sess.run(cost, feed_dict={X: train_X, Y:train_Y})), \                &quot;W=&quot;, sess.run(W), &quot;b=&quot;, sess.run(b)    print &quot;Optimization Finished!&quot;    print &quot;cost=&quot;, sess.run(cost, feed_dict={X: train_X, Y: train_Y}), \          &quot;W=&quot;, sess.run(W), &quot;b=&quot;, sess.run(b)    #Graphic display    plt.plot(train_X, train_Y, &#39;ro&#39;, label=&#39;Original data&#39;)    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label=&#39;Fitted line&#39;)    plt.legend()    plt.show()</code></pre><p><img src="http:#upload-images.jianshu.io/upload_images/4110803-37152f7aa4bd92f7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>逻辑回归</p><pre><code>import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets(&quot;/tmp/data/&quot;, one_hot=True)# Parameterslearning_rate = 0.01training_epochs = 25batch_size = 100display_step = 1# tf Graph Inputx = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition =&gt; 10 classes# Set model weightsW = tf.Variable(tf.zeros([784, 10]))b = tf.Variable(tf.zeros([10]))# Construct modelpred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax# Minimize error using cross entropycost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))# Gradient Descentoptimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)# Initializing the variablesinit = tf.initialize_all_variables()# Launch the graphwith tf.Session() as sess:    sess.run(init)    # Training cycle    for epoch in range(training_epochs):        avg_cost = 0.        total_batch = int(mnist.train.num_examples/batch_size)        # Loop over all batches        for i in range(total_batch):            batch_xs, batch_ys = mnist.train.next_batch(batch_size)            # Run optimization op (backprop) and cost op (to get loss value)            _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,                                                          y: batch_ys})            # Compute average loss            avg_cost += c / total_batch        # Display logs per epoch step        if (epoch+1) % display_step == 0:            print &quot;Epoch:&quot;, &#39;%04d&#39; % (epoch+1), &quot;cost=&quot;, &quot;{:.9f}&quot;.format(avg_cost)    print &quot;Optimization Finished!&quot;    # Test model    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))    # Calculate accuracy    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))    print &quot;Accuracy:&quot;, accuracy.eval({x: mnist.test.images, y: mnist.test.labels})    # result :    Epoch: 0001 cost= 29.860467369    Epoch: 0002 cost= 22.001451784    Epoch: 0003 cost= 21.019925554    Epoch: 0004 cost= 20.561320320    Epoch: 0005 cost= 20.109135756    Epoch: 0006 cost= 19.927862290    Epoch: 0007 cost= 19.548687116    Epoch: 0008 cost= 19.429119071    Epoch: 0009 cost= 19.397068211    Epoch: 0010 cost= 19.180813479    Epoch: 0011 cost= 19.026808132    Epoch: 0012 cost= 19.057875510    Epoch: 0013 cost= 19.009575057    Epoch: 0014 cost= 18.873240641    Epoch: 0015 cost= 18.718575359    Epoch: 0016 cost= 18.718761925    Epoch: 0017 cost= 18.673640560    Epoch: 0018 cost= 18.562128253    Epoch: 0019 cost= 18.458205289    Epoch: 0020 cost= 18.538211225    Epoch: 0021 cost= 18.443384213    Epoch: 0022 cost= 18.428727668    Epoch: 0023 cost= 18.304270616    Epoch: 0024 cost= 18.323529782    Epoch: 0025 cost= 18.247192113    Optimization Finished!    (10000, 784)    Accuracy 0.9206</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在本教程中，我将会解释：&lt;/p&gt;
&lt;p&gt;1.TensorFlow模型是什么样的？&lt;/p&gt;
&lt;p&gt;2.如何保存TensorFlow模型？&lt;/p&gt;
&lt;p&gt;3.如何恢复预测/转移学习的TensorFlow模型？&lt;/p&gt;
&lt;p&gt;4.如何使用导入的预先训练的模型进行微调和修改?&lt;/p&gt;
&lt;p&gt;5.这个教程假设你已经对神经网络有了一定的了解。如果不了解的话请查阅相关资料&lt;/p&gt;
    
    </summary>
    
      <category term="Tensorflow" scheme="http://yoursite.com/categories/Tensorflow/"/>
    
    
      <category term="Tensorflow" scheme="http://yoursite.com/tags/Tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>深圳独立坐标转换为WGS84</title>
    <link href="http://yoursite.com/2018/09/26/%E5%9D%90%E6%A0%87%E8%BD%AC%E6%8D%A2%E8%AE%B0%E5%BD%95/"/>
    <id>http://yoursite.com/2018/09/26/坐标转换记录/</id>
    <published>2018-09-26T05:06:18.000Z</published>
    <updated>2018-09-26T05:07:02.147Z</updated>
    
    <content type="html"><![CDATA[<p>方旭</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;方旭&lt;/p&gt;

      
    
    </summary>
    
      <category term="测绘工程" scheme="http://yoursite.com/categories/%E6%B5%8B%E7%BB%98%E5%B7%A5%E7%A8%8B/"/>
    
    
      <category term="坐标" scheme="http://yoursite.com/tags/%E5%9D%90%E6%A0%87/"/>
    
  </entry>
  
  <entry>
    <title>Effective Use of Dilated Convolutions for Segmenting Small Object Instances in Remote Sensing Imagery</title>
    <link href="http://yoursite.com/2018/07/24/Imagery/"/>
    <id>http://yoursite.com/2018/07/24/Imagery/</id>
    <published>2018-07-24T13:25:13.000Z</published>
    <updated>2018-07-24T13:28:05.000Z</updated>
    
    <content type="html"><![CDATA[<p>摘要：Thanks to recent advances in CNNs, solid improvements have been made in semantic segmentation of high resolution remote sensing imagery. However, most of the previous works have not fully taken into account the specific<a id="more"></a> difficulties that exist in remote sensing tasks. One of such difficulties is that objects are small and crowded in remote sensing imagery. To tackle with this challenging task we have proposed a novel architecture called local feature extraction (LFE) module attached on top of dilated front-end module. The LFE module is based on our findings that aggressively increasing dilation factors fails to aggregate local features due to sparsity of the kernel, and detrimental to small objects. The proposed LFE module solves this problem by aggregating local features with decreasing dilation factor. We tested our network on three remote sensing datasets and acquired remarkably good results for all datasets especially for small objects.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;摘要：Thanks to recent advances in CNNs, solid improvements have been made in semantic segmentation of high resolution remote sensing imagery. However, most of the previous works have not fully taken into account the specific
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>tensorflow 从入门到上天(一)</title>
    <link href="http://yoursite.com/2017/08/24/tensorflow-%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E4%B8%8A%E5%A4%A9-%E4%B8%80/"/>
    <id>http://yoursite.com/2017/08/24/tensorflow-从入门到上天-一/</id>
    <published>2017-08-24T02:14:13.000Z</published>
    <updated>2017-08-24T06:08:22.000Z</updated>
    
    <content type="html"><![CDATA[<p>安装配置什么的不讲了，就是安装英伟达驱动，CUDA，坑也比较多，值得专门写一篇，值得注意一点。tensorflow从1.0之后好像要求CUDA8.0，直接跑hello world 代码 开始；</p><a id="more"></a><pre><code>import tensorflow as tfhello = tf.constant(&#39;Hello, TensorFlow!&#39;)/*fangxu */sess = tf.Session()print sess.run(hello)</code></pre><p>首先，通过tf.constant创建一个常量，然后启动Tensorflow的Session，调用sess的run方法来启动整个graph。 接下来我们做下简单的数学的方法：</p><pre><code>import tensorflow as tfa = tf.constant(2)b = tf.constant(3)with tf.Session() as sess:    print &quot;a=2, b=3&quot;    print &quot;Addition with constants: %i&quot; % sess.run(a+b)    print &quot;Multiplication with constants: %i&quot; % sess.run(a*b)</code></pre><p>tensorflow中保存数据的利器，placeholder（type,strucuct…)它的第一个参数是你要保存的数据的数据类型，大多数是tensorflow中的float32数据类型，后面的参数就是要保存数据的结构，比如要保存一个1×2的矩阵，则struct=[1 2]。它在使用的时候和前面的variable不同的是在session运行阶段，需要给placeholder提供数据，利用feed_dict的字典结构给placeholdr变量“喂数据”，具体使用如下</p><p>placeholder 使用</p><pre><code>import tensorflow as tfa = tf.placeholder(tf.int16)b = tf.placeholder(tf.int16)add = tf.add(a, b)mul = tf.multiply(a, b)with tf.Session() as sess:    # Run every operation with variable input    print &quot;Addition with variables: %i&quot; % sess.run(add, feed_dict={a: 2, b: 3})    print &quot;Multiplication with variables: %i&quot; % sess.run(mul, feed_dict={a: 2, b: 3})# output:Addition with variables: 5Multiplication with variables: 6*/matrix1 = tf.constant([[3., 3.]])matrix2 = tf.constant([[2.],[2.]])product=tf.matmul(matrix1,matrix2)with tf.Session() as sess:    result = sess.run(product)    print result    #result:    [[12.]]</code></pre><p>注：<br><strong>Tensorflow中废弃的API及替代<br>tf.mul  tf.sub   tf.neg 已经废弃<br>分别可用tf.multiply  tf.subtract  tf.negative替代.</strong></p><p>线性回归:以下代码来自<a href="https:#github.com/aymericdamien/TensorFlow-Examples/" target="_blank" rel="noopener">https:#github.com/aymericdamien/TensorFlow-Examples/</a><br>仅作学习用</p><pre><code>import tensorflow as tfimport numpyimport matplotlib.pyplot as pltrng = numpy.randomlearning_rate = 0.01training_epochs = 2000display_step = 50# Training Datatrain_X = numpy.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,7.042,10.791,5.313,7.997,5.654,9.27,3.1])train_Y = numpy.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,2.827,3.465,1.65,2.904,2.42,2.94,1.3])n_samples = train_X.shape[0]#tf Graph InputX = tf.placeholder(&quot;float&quot;)Y = tf.placeholder(&quot;float&quot;)#Create Model#Set model weightsW = tf.Variable(rng.randn(), name=&quot;weight&quot;)b = tf.Variable(rng.randn(), name=&quot;bias&quot;)#Construct a linear modelactivation = tf.add(tf.multiply(X, W), b)# Minimize the squared errorscost = tf.reduce_sum(tf.pow(activation-Y, 2))/(2*n_samples) #L2 lossoptimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost) #Gradient descent# Initializing the variablesinit = tf.initialize_all_variables()# Launch the graphwith tf.Session() as sess:    sess.run(init)    # Fit all training data    for epoch in range(training_epochs):        for (x, y) in zip(train_X, train_Y):            sess.run(optimizer, feed_dict={X: x, Y: y})        #Display logs per epoch step        if epoch % display_step == 0:            print &quot;Epoch:&quot;, &#39;%04d&#39; % (epoch+1), &quot;cost=&quot;, \                &quot;{:.9f}&quot;.format(sess.run(cost, feed_dict={X: train_X, Y:train_Y})), \                &quot;W=&quot;, sess.run(W), &quot;b=&quot;, sess.run(b)    print &quot;Optimization Finished!&quot;    print &quot;cost=&quot;, sess.run(cost, feed_dict={X: train_X, Y: train_Y}), \          &quot;W=&quot;, sess.run(W), &quot;b=&quot;, sess.run(b)    #Graphic display    plt.plot(train_X, train_Y, &#39;ro&#39;, label=&#39;Original data&#39;)    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label=&#39;Fitted line&#39;)    plt.legend()    plt.show()</code></pre><p><img src="http:#upload-images.jianshu.io/upload_images/4110803-37152f7aa4bd92f7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>逻辑回归</p><pre><code>import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets(&quot;/tmp/data/&quot;, one_hot=True)# Parameterslearning_rate = 0.01training_epochs = 25batch_size = 100display_step = 1# tf Graph Inputx = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition =&gt; 10 classes# Set model weightsW = tf.Variable(tf.zeros([784, 10]))b = tf.Variable(tf.zeros([10]))# Construct modelpred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax# Minimize error using cross entropycost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))# Gradient Descentoptimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)# Initializing the variablesinit = tf.initialize_all_variables()# Launch the graphwith tf.Session() as sess:    sess.run(init)    # Training cycle    for epoch in range(training_epochs):        avg_cost = 0.        total_batch = int(mnist.train.num_examples/batch_size)        # Loop over all batches        for i in range(total_batch):            batch_xs, batch_ys = mnist.train.next_batch(batch_size)            # Run optimization op (backprop) and cost op (to get loss value)            _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,                                                          y: batch_ys})            # Compute average loss            avg_cost += c / total_batch        # Display logs per epoch step        if (epoch+1) % display_step == 0:            print &quot;Epoch:&quot;, &#39;%04d&#39; % (epoch+1), &quot;cost=&quot;, &quot;{:.9f}&quot;.format(avg_cost)    print &quot;Optimization Finished!&quot;    # Test model    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))    # Calculate accuracy    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))    print &quot;Accuracy:&quot;, accuracy.eval({x: mnist.test.images, y: mnist.test.labels})    # result :    Epoch: 0001 cost= 29.860467369    Epoch: 0002 cost= 22.001451784    Epoch: 0003 cost= 21.019925554    Epoch: 0004 cost= 20.561320320    Epoch: 0005 cost= 20.109135756    Epoch: 0006 cost= 19.927862290    Epoch: 0007 cost= 19.548687116    Epoch: 0008 cost= 19.429119071    Epoch: 0009 cost= 19.397068211    Epoch: 0010 cost= 19.180813479    Epoch: 0011 cost= 19.026808132    Epoch: 0012 cost= 19.057875510    Epoch: 0013 cost= 19.009575057    Epoch: 0014 cost= 18.873240641    Epoch: 0015 cost= 18.718575359    Epoch: 0016 cost= 18.718761925    Epoch: 0017 cost= 18.673640560    Epoch: 0018 cost= 18.562128253    Epoch: 0019 cost= 18.458205289    Epoch: 0020 cost= 18.538211225    Epoch: 0021 cost= 18.443384213    Epoch: 0022 cost= 18.428727668    Epoch: 0023 cost= 18.304270616    Epoch: 0024 cost= 18.323529782    Epoch: 0025 cost= 18.247192113    Optimization Finished!    (10000, 784)    Accuracy 0.9206</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;安装配置什么的不讲了，就是安装英伟达驱动，CUDA，坑也比较多，值得专门写一篇，值得注意一点。tensorflow从1.0之后好像要求CUDA8.0，直接跑hello world 代码 开始；&lt;/p&gt;
    
    </summary>
    
      <category term="Tensorflow" scheme="http://yoursite.com/categories/Tensorflow/"/>
    
    
      <category term="Tensorflow" scheme="http://yoursite.com/tags/Tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>MarkDown 语法学习与使用</title>
    <link href="http://yoursite.com/2017/08/22/MarkDown-%E8%AF%AD%E6%B3%95%E5%AD%A6%E4%B9%A0%E4%B8%8E%E4%BD%BF%E7%94%A8/"/>
    <id>http://yoursite.com/2017/08/22/MarkDown-语法学习与使用/</id>
    <published>2017-08-22T01:57:48.000Z</published>
    <updated>2017-08-22T04:16:55.000Z</updated>
    
    <content type="html"><![CDATA[<p><font size="4">MarkDown 从入门到上天</font></p><a id="more"></a><h1 id="markdown语法使用"><a href="#markdown语法使用" class="headerlink" title="markdown语法使用"></a>markdown语法使用</h1><h2 id="二级标题"><a href="#二级标题" class="headerlink" title="二级标题"></a>二级标题</h2><p>fangxu</p><p>平常文字</p><blockquote><p>这是一个应用</p></blockquote><h3 id="一、主要内容"><a href="#一、主要内容" class="headerlink" title="一、主要内容"></a>一、主要内容</h3><p>无序列表</p><ul><li>方旭</li><li>kkfang</li><li>yi yi yi</li></ul><p>有序列表</p><ol><li>方旭</li><li>方娜</li><li>廖雅诗</li></ol><p><em>斜体语法</em><br><strong>加粗强调</strong></p><pre><code>这是代码区块void main() {    cout&lt;&lt;&quot;hello world!&quot;&lt;&lt;endl; }</code></pre><hr><center>![fangxu](http://upload-images.jianshu.io/upload_images/4110803-0eb2ded2889218d1.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/200 "图片描述")</center>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;font size=&quot;4&quot;&gt;MarkDown 从入门到上天&lt;/font&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="MarkDown" scheme="http://yoursite.com/categories/MarkDown/"/>
    
    
      <category term="MarkDown" scheme="http://yoursite.com/tags/MarkDown/"/>
    
  </entry>
  
  <entry>
    <title>fangxu</title>
    <link href="http://yoursite.com/2017/05/21/fangxu/"/>
    <id>http://yoursite.com/2017/05/21/fangxu/</id>
    <published>2017-05-21T09:49:25.000Z</published>
    <updated>2017-08-21T06:21:19.000Z</updated>
    
    <content type="html"><![CDATA[<p>部署<br>npm install hexo-deployer-git –save</p><p>hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;部署&lt;br&gt;npm install hexo-deployer-git –save&lt;/p&gt;
&lt;p&gt;hexo clean &amp;amp;&amp;amp; hexo g &amp;amp;&amp;amp; hexo d&lt;/p&gt;

      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Hexo 使用笔记</title>
    <link href="http://yoursite.com/2017/05/21/Hexo-%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2017/05/21/Hexo-使用笔记/</id>
    <published>2017-05-21T09:49:25.000Z</published>
    <updated>2018-09-21T14:14:14.218Z</updated>
    
    <content type="html"><![CDATA[<h3 id="发表文章"><a href="#发表文章" class="headerlink" title="发表文章"></a>发表文章</h3><pre><code>hexo new post &lt;title&gt;</code></pre><h3 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h3><h4 id="部署1"><a href="#部署1" class="headerlink" title="部署1"></a>部署1</h4><pre><code>npm install hexo-deployer-git --savehexo clean &amp;&amp; hexo g &amp;&amp; hexo d</code></pre><h4 id="部署2"><a href="#部署2" class="headerlink" title="部署2"></a>部署2</h4><h3 id="git配置"><a href="#git配置" class="headerlink" title="git配置"></a>git配置</h3><p>$$ \left(x-1\right)\left(x+3\right) $$</p><p>如何在markdownpad 插入公式</p><p>1.下载<a href="https://github.com/zohooo/jaxedit/releases" target="_blank" rel="noopener">jaxedit</a>文件，解压，找到如下路径 jaxedit-0.30\library\mathjax\unpacked\MathJax.js</p><p>2.转到MarkdownPad2中，点击<strong>Tools &gt; Options &gt; Advanced &gt; HTML Head Editor</strong>，输入：</p><pre><code>&lt;script type=&quot;text/javascript&quot;  src=&quot;D:\jaxedit-0.30\library\mathjax\unpacked\MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;&lt;/script&gt;</code></pre><h4 id="在hexo博客-使用latex-公式"><a href="#在hexo博客-使用latex-公式" class="headerlink" title="在hexo博客 使用latex 公式"></a>在hexo博客 使用latex 公式</h4><p>1.更换渲染引擎来支持mathjax</p><pre><code>npm uninstall hexo-renderer-marked --savenpm install hexo-renderer-kramed --save</code></pre><p>安装Kramed</p><p>2 更改文件配置</p><p>打开<code>/node_modules/hexo-renderer-kramed/lib/renderer.js</code>，更改：</p><pre><code>// Change inline math rulefunction formatText(text) {    // Fit kramed&#39;s rule: $$ + \1 + $$    return text.replace(/`\$(.*?)\$`/g, &#39;$$$$$1$$$$&#39;);}为，直接返回text// Change inline math rulefunction formatText(text) {    return text;}</code></pre><p>3 停止使用hexo-math,安装mathjax 包</p><pre><code>npm uninstall hexo-math --savenpm install hexo-renderer-mathjax --save</code></pre><p>4 更新mathjax 的配置文件</p><p>打开<code>/node_modules/hexo-renderer-mathjax/mathjax.html</code></p><p>更改<code>&lt;script&gt;</code>为以下代码</p><pre><code>&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;&lt;/script&gt;</code></pre><p>5 更改默认转义规则</p><p> 因为LaTex 与markdown 语法有语义冲突，所以hexo默认转义规则会将一些字符进行转义，需要对默认的规则进行修改</p><p>1.打开<code>/node_modules/kramed/lib/rules/inline.js</code></p><pre><code>escape: /^\\([\\`*{}\[\]()#$+\-.!_&gt;])/,</code></pre><p>更改为</p><pre><code>escape: /^\\([`*\[\]()# +\-.!_&gt;])/,</code></pre><p>2.</p><pre><code>em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</code></pre><p>更改为</p><pre><code>em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;发表文章&quot;&gt;&lt;a href=&quot;#发表文章&quot; class=&quot;headerlink&quot; title=&quot;发表文章&quot;&gt;&lt;/a&gt;发表文章&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;hexo new post &amp;lt;title&amp;gt;&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;部署&quot;&gt;&lt;a
      
    
    </summary>
    
      <category term="Hexo" scheme="http://yoursite.com/categories/Hexo/"/>
    
    
      <category term="Hexo" scheme="http://yoursite.com/tags/Hexo/"/>
    
  </entry>
  
  <entry>
    <title>articletitle</title>
    <link href="http://yoursite.com/2017/05/21/articletitle/"/>
    <id>http://yoursite.com/2017/05/21/articletitle/</id>
    <published>2017-05-21T09:38:59.000Z</published>
    <updated>2017-05-21T09:38:59.000Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
</feed>
